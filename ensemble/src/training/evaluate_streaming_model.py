"""
ä¸“é—¨çš„æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”è„šæœ¬
"""
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
sys.path.append(str(project_root))

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from tqdm import tqdm

from ensemble.src.training.streaming_multi_rocket import StreamingMultiRocketClassifier, StreamingInferenceEngine
from ensemble.src.training.data_loader import TrajectoryDataLoader


def load_trained_model(checkpoint_path: str, device: str = 'auto'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if device == 'auto':
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ä» {checkpoint_path} åŠ è½½æ¨¡å‹...")
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # é‡å»ºæ¨¡å‹
    data_info = checkpoint['data_info']
    config = checkpoint.get('config')
    
    model = StreamingMultiRocketClassifier(
        c_in=data_info['num_features'],
        c_out=data_info['num_classes'],
        max_seq_len=data_info['seq_len'],
        min_seq_len=getattr(config, 'min_seq_len', 10),
        device=device,
        confidence_threshold=getattr(config, 'confidence_threshold', 0.7)
    ).to(device)
    
    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"   æ”¯æŒçš„åºåˆ—é•¿åº¦: {model.supported_lengths}")
    print(f"   æ•°æ®ä¿¡æ¯: {data_info}")
    
    return model, checkpoint


def evaluate_streaming_vs_batch(model, data_loader, device, detailed_analysis=True):
    """å¯¹æ¯”æµå¼æ¨ç†å’Œæ‰¹é‡æ¨ç†çš„æ€§èƒ½"""
    print("\\nğŸ“Š æµå¼æ¨ç† vs æ‰¹é‡æ¨ç†å¯¹æ¯”è¯„ä¼°")
    print("=" * 50)
    
    model.eval()
    
    # å­˜å‚¨ç»“æœ
    batch_results = {
        'predictions': [],
        'targets': [],
        'confidences': [],
        'inference_times': []
    }
    
    streaming_results = {
        'predictions': [],
        'targets': [],
        'confidences': [],
        'early_stops': [],
        'stop_timesteps': [],
        'inference_times': [],
        'timestep_predictions': []  # æ¯ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹å†å²
    }
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(data_loader, desc="è¯„ä¼°ä¸­")):
            sequences = batch['sequences'].to(device)  # (batch, seq_len, features)
            labels = batch['labels'].to(device)
            
            # è½¬æ¢ç»´åº¦
            sequences_t = sequences.transpose(1, 2)  # (batch, features, seq_len)
            
            # æ‰¹é‡æ¨ç†
            import time
            start_time = time.time()
            batch_output = model.forward(sequences_t)
            batch_time = time.time() - start_time
            
            batch_preds = torch.argmax(batch_output['logits'], dim=1)
            batch_confs = batch_output['confidence']
            
            batch_results['predictions'].extend(batch_preds.cpu().numpy())
            batch_results['targets'].extend(labels.cpu().numpy())
            batch_results['confidences'].extend(batch_confs.cpu().numpy())
            batch_results['inference_times'].append(batch_time / sequences.shape[0])  # æ¯æ ·æœ¬æ—¶é—´
            
            # æµå¼æ¨ç†
            for i in range(sequences.shape[0]):
                seq = sequences[i].numpy()  # (seq_len, features)
                true_label = labels[i].item()
                
                engine = StreamingInferenceEngine(model)
                
                start_time = time.time()
                timestep_preds = []
                final_pred = None
                stopped_early = False
                stop_step = len(seq)
                
                # é€æ­¥æ·»åŠ æ—¶é—´æ­¥
                for t in range(len(seq)):
                    features = seq[t]
                    result = engine.add_timestep(features)
                    
                    timestep_preds.append({
                        'timestep': t + 1,
                        'prediction': result['prediction'],
                        'confidence': result['confidence']
                    })
                    
                    if result['stopped_early'] and not stopped_early:
                        final_pred = result['prediction']
                        stopped_early = True
                        stop_step = t + 1
                        break
                
                streaming_time = time.time() - start_time
                
                if final_pred is None:
                    final_pred = result['prediction']
                
                streaming_results['predictions'].append(final_pred)
                streaming_results['targets'].append(true_label)
                streaming_results['confidences'].append(result['confidence'])
                streaming_results['early_stops'].append(stopped_early)
                streaming_results['stop_timesteps'].append(stop_step)
                streaming_results['inference_times'].append(streaming_time)
                streaming_results['timestep_predictions'].append(timestep_preds)
            
            # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡ä»¥èŠ‚çœæ—¶é—´
            if batch_idx >= 10:  # åªè¯„ä¼°å‰å‡ ä¸ªbatch
                break
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    batch_acc = accuracy_score(batch_results['targets'], batch_results['predictions'])
    streaming_acc = accuracy_score(streaming_results['targets'], streaming_results['predictions'])
    
    batch_prec, batch_rec, batch_f1, _ = precision_recall_fscore_support(
        batch_results['targets'], batch_results['predictions'], average='weighted'
    )
    streaming_prec, streaming_rec, streaming_f1, _ = precision_recall_fscore_support(
        streaming_results['targets'], streaming_results['predictions'], average='weighted'
    )
    
    # æ—¶é—´ç»Ÿè®¡
    avg_batch_time = np.mean(batch_results['inference_times']) * 1000  # ms
    avg_streaming_time = np.mean(streaming_results['inference_times']) * 1000  # ms
    
    # æ—©æœŸåœæ­¢ç»Ÿè®¡
    early_stop_rate = np.mean(streaming_results['early_stops'])
    avg_stop_timestep = np.mean(streaming_results['stop_timesteps'])
    max_timesteps = max(streaming_results['stop_timesteps'])
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    avg_batch_conf = np.mean(batch_results['confidences'])
    avg_streaming_conf = np.mean(streaming_results['confidences'])
    
    print("\\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"{'æŒ‡æ ‡':<20} {'æ‰¹é‡æ¨ç†':<15} {'æµå¼æ¨ç†':<15} {'å·®å¼‚':<10}")
    print("-" * 65)
    print(f"{'å‡†ç¡®ç‡':<20} {batch_acc:<15.4f} {streaming_acc:<15.4f} {streaming_acc-batch_acc:<+10.4f}")
    print(f"{'ç²¾ç¡®ç‡':<20} {batch_prec:<15.4f} {streaming_prec:<15.4f} {streaming_prec-batch_prec:<+10.4f}")
    print(f"{'å¬å›ç‡':<20} {batch_rec:<15.4f} {streaming_rec:<15.4f} {streaming_rec-batch_rec:<+10.4f}")
    print(f"{'F1åˆ†æ•°':<20} {batch_f1:<15.4f} {streaming_f1:<15.4f} {streaming_f1-batch_f1:<+10.4f}")
    print(f"{'å¹³å‡ç½®ä¿¡åº¦':<20} {avg_batch_conf:<15.4f} {avg_streaming_conf:<15.4f} {avg_streaming_conf-avg_batch_conf:<+10.4f}")
    print(f"{'æ¨ç†æ—¶é—´(ms)':<20} {avg_batch_time:<15.2f} {avg_streaming_time:<15.2f} {avg_streaming_time-avg_batch_time:<+10.2f}")
    
    print(f"\\nğŸ¯ æµå¼æ¨ç†ç‰¹æœ‰æŒ‡æ ‡:")
    print(f"   æ—©æœŸåœæ­¢ç‡: {early_stop_rate:.2%}")
    print(f"   å¹³å‡åœæ­¢æ—¶é—´æ­¥: {avg_stop_timestep:.1f} / {max_timesteps}")
    print(f"   æ—¶é—´æ­¥èŠ‚çœ: {(max_timesteps - avg_stop_timestep) / max_timesteps:.2%}")
    
    if detailed_analysis:
        # è¯¦ç»†åˆ†æ
        analyze_early_stopping_patterns(streaming_results)
        plot_streaming_analysis(streaming_results, batch_results)
    
    return {
        'batch_results': batch_results,
        'streaming_results': streaming_results,
        'metrics': {
            'batch_accuracy': batch_acc,
            'streaming_accuracy': streaming_acc,
            'early_stop_rate': early_stop_rate,
            'avg_stop_timestep': avg_stop_timestep,
            'time_speedup': avg_batch_time / avg_streaming_time if avg_streaming_time > 0 else 1.0
        }
    }


def analyze_early_stopping_patterns(streaming_results):
    """åˆ†ææ—©æœŸåœæ­¢æ¨¡å¼"""
    print("\\nğŸ” æ—©æœŸåœæ­¢æ¨¡å¼åˆ†æ:")
    
    stop_timesteps = np.array(streaming_results['stop_timesteps'])
    early_stops = np.array(streaming_results['early_stops'])
    
    # æŒ‰æ˜¯å¦æ—©æœŸåœæ­¢åˆ†ç»„
    early_stop_indices = np.where(early_stops)[0]
    no_early_stop_indices = np.where(~early_stops)[0]
    
    if len(early_stop_indices) > 0:
        early_stop_timesteps = stop_timesteps[early_stop_indices]
        print(f"   æ—©æœŸåœæ­¢æ ·æœ¬æ•°: {len(early_stop_indices)}")
        print(f"   æ—©æœŸåœæ­¢æ—¶é—´æ­¥åˆ†å¸ƒ: å¹³å‡={np.mean(early_stop_timesteps):.1f}, "
              f"ä¸­ä½æ•°={np.median(early_stop_timesteps):.1f}, "
              f"èŒƒå›´=[{np.min(early_stop_timesteps)}, {np.max(early_stop_timesteps)}]")
        
        # æ—©æœŸåœæ­¢çš„å‡†ç¡®ç‡
        early_stop_preds = np.array(streaming_results['predictions'])[early_stop_indices]
        early_stop_targets = np.array(streaming_results['targets'])[early_stop_indices]
        early_stop_acc = accuracy_score(early_stop_targets, early_stop_preds)
        print(f"   æ—©æœŸåœæ­¢æ ·æœ¬å‡†ç¡®ç‡: {early_stop_acc:.4f}")
    
    if len(no_early_stop_indices) > 0:
        no_early_stop_preds = np.array(streaming_results['predictions'])[no_early_stop_indices]
        no_early_stop_targets = np.array(streaming_results['targets'])[no_early_stop_indices]
        no_early_stop_acc = accuracy_score(no_early_stop_targets, no_early_stop_preds)
        print(f"   å®Œæ•´åºåˆ—æ ·æœ¬å‡†ç¡®ç‡: {no_early_stop_acc:.4f}")
    
    # æŒ‰ç±»åˆ«åˆ†æ
    targets = np.array(streaming_results['targets'])
    unique_classes = np.unique(targets)
    
    print("\\n   å„ç±»åˆ«æ—©æœŸåœæ­¢ç‡:")
    for cls in unique_classes:
        cls_indices = np.where(targets == cls)[0]
        cls_early_stops = early_stops[cls_indices]
        cls_early_rate = np.mean(cls_early_stops)
        print(f"     ç±»åˆ« {cls}: {cls_early_rate:.2%} ({np.sum(cls_early_stops)}/{len(cls_early_stops)})")


def plot_streaming_analysis(streaming_results, batch_results, suffix=""):
    """ç»˜åˆ¶æµå¼æ¨ç†åˆ†æå›¾è¡¨"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. å‡†ç¡®ç‡å¯¹æ¯”
    batch_acc = accuracy_score(batch_results['targets'], batch_results['predictions'])
    streaming_acc = accuracy_score(streaming_results['targets'], streaming_results['predictions'])
    
    axes[0, 0].bar(['æ‰¹é‡æ¨ç†', 'æµå¼æ¨ç†'], [batch_acc, streaming_acc], 
                   color=['blue', 'green'], alpha=0.7)
    axes[0, 0].set_title('å‡†ç¡®ç‡å¯¹æ¯”')
    axes[0, 0].set_ylabel('å‡†ç¡®ç‡')
    axes[0, 0].set_ylim(0, 1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate([batch_acc, streaming_acc]):
        axes[0, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')
    
    # 2. æ—©æœŸåœæ­¢æ—¶é—´æ­¥åˆ†å¸ƒ
    stop_timesteps = streaming_results['stop_timesteps']
    axes[0, 1].hist(stop_timesteps, bins=20, alpha=0.7, color='orange', edgecolor='black')
    axes[0, 1].axvline(np.mean(stop_timesteps), color='red', linestyle='--', 
                       label=f'å¹³å‡: {np.mean(stop_timesteps):.1f}')
    axes[0, 1].set_title('åœæ­¢æ—¶é—´æ­¥åˆ†å¸ƒ')
    axes[0, 1].set_xlabel('åœæ­¢æ—¶é—´æ­¥')
    axes[0, 1].set_ylabel('æ ·æœ¬æ•°é‡')
    axes[0, 1].legend()
    
    # 3. ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”
    axes[0, 2].hist(batch_results['confidences'], bins=30, alpha=0.5, 
                    label='æ‰¹é‡æ¨ç†', color='blue', density=True)
    axes[0, 2].hist(streaming_results['confidences'], bins=30, alpha=0.5, 
                    label='æµå¼æ¨ç†', color='green', density=True)
    axes[0, 2].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”')
    axes[0, 2].set_xlabel('ç½®ä¿¡åº¦')
    axes[0, 2].set_ylabel('å¯†åº¦')
    axes[0, 2].legend()
    
    # 4. æ··æ·†çŸ©é˜µå¯¹æ¯” - æ‰¹é‡æ¨ç†
    cm_batch = confusion_matrix(batch_results['targets'], batch_results['predictions'])
    sns.heatmap(cm_batch, annot=True, fmt='d', ax=axes[1, 0], cmap='Blues')
    axes[1, 0].set_title('æ‰¹é‡æ¨ç†æ··æ·†çŸ©é˜µ')
    axes[1, 0].set_xlabel('é¢„æµ‹ç±»åˆ«')
    axes[1, 0].set_ylabel('çœŸå®ç±»åˆ«')
    
    # 5. æ··æ·†çŸ©é˜µå¯¹æ¯” - æµå¼æ¨ç†  
    cm_streaming = confusion_matrix(streaming_results['targets'], streaming_results['predictions'])
    sns.heatmap(cm_streaming, annot=True, fmt='d', ax=axes[1, 1], cmap='Greens')
    axes[1, 1].set_title('æµå¼æ¨ç†æ··æ·†çŸ©é˜µ')
    axes[1, 1].set_xlabel('é¢„æµ‹ç±»åˆ«')
    axes[1, 1].set_ylabel('çœŸå®ç±»åˆ«')
    
    # 6. æ—©æœŸåœæ­¢vså‡†ç¡®ç‡æ•£ç‚¹å›¾
    early_stops = np.array(streaming_results['early_stops'])
    targets = np.array(streaming_results['targets'])
    predictions = np.array(streaming_results['predictions'])
    correct = (targets == predictions)
    
    # åˆ†åˆ«ç»˜åˆ¶æ—©æœŸåœæ­¢å’Œéæ—©æœŸåœæ­¢çš„æ ·æœ¬
    early_indices = np.where(early_stops)[0]
    no_early_indices = np.where(~early_stops)[0]
    
    if len(early_indices) > 0:
        axes[1, 2].scatter(np.array(stop_timesteps)[early_indices], 
                          correct[early_indices].astype(int),
                          c='red', alpha=0.6, label='æ—©æœŸåœæ­¢', s=30)
    
    if len(no_early_indices) > 0:
        axes[1, 2].scatter(np.array(stop_timesteps)[no_early_indices], 
                          correct[no_early_indices].astype(int),
                          c='blue', alpha=0.6, label='å®Œæ•´åºåˆ—', s=30)
    
    axes[1, 2].set_title('åœæ­¢æ—¶é—´æ­¥ vs é¢„æµ‹æ­£ç¡®æ€§')
    axes[1, 2].set_xlabel('åœæ­¢æ—¶é—´æ­¥')
    axes[1, 2].set_ylabel('é¢„æµ‹æ­£ç¡® (1=æ­£ç¡®, 0=é”™è¯¯)')
    axes[1, 2].legend()
    axes[1, 2].set_ylim(-0.1, 1.1)
    
    plt.tight_layout()
    plt.savefig(f'streaming_analysis{suffix}.png', dpi=150, bbox_inches='tight')
    print(f"\\nğŸ“Š åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ° streaming_analysis{suffix}.png")


def analyze_timestep_evolution(streaming_results, max_samples=5):
    """åˆ†æé¢„æµ‹éšæ—¶é—´æ­¥çš„æ¼”åŒ–"""
    print(f"\\nğŸ“ˆ é¢„æµ‹æ¼”åŒ–åˆ†æ (å‰{max_samples}ä¸ªæ ·æœ¬):")
    
    timestep_data = streaming_results['timestep_predictions'][:max_samples]
    targets = streaming_results['targets'][:max_samples]
    
    fig, axes = plt.subplots(max_samples, 1, figsize=(12, 3*max_samples))
    if max_samples == 1:
        axes = [axes]
    
    for i, (sample_data, true_label) in enumerate(zip(timestep_data, targets)):
        timesteps = [d['timestep'] for d in sample_data]
        predictions = [d['prediction'] for d in sample_data]
        confidences = [d['confidence'] for d in sample_data]
        
        # ç»˜åˆ¶é¢„æµ‹å˜åŒ–
        ax1 = axes[i]
        ax1.step(timesteps, predictions, 'b-', where='post', linewidth=2, label='é¢„æµ‹ç±»åˆ«')
        ax1.axhline(y=true_label, color='red', linestyle='--', alpha=0.7, label=f'çœŸå®ç±»åˆ« ({true_label})')
        ax1.set_ylabel('é¢„æµ‹ç±»åˆ«')
        ax1.set_title(f'æ ·æœ¬ {i+1} - é¢„æµ‹æ¼”åŒ–')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶ç½®ä¿¡åº¦
        ax2 = ax1.twinx()
        ax2.plot(timesteps, confidences, 'g-', alpha=0.7, label='ç½®ä¿¡åº¦')
        ax2.axhline(y=0.7, color='orange', linestyle=':', alpha=0.7, label='ç½®ä¿¡åº¦é˜ˆå€¼')
        ax2.set_ylabel('ç½®ä¿¡åº¦')
        ax2.legend(loc='upper right')
        ax2.set_ylim(0, 1)
        
        if i == max_samples - 1:
            ax1.set_xlabel('æ—¶é—´æ­¥')
    
    plt.tight_layout()
    plt.savefig('prediction_evolution.png', dpi=150, bbox_inches='tight')
    print("ğŸ“Š é¢„æµ‹æ¼”åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° prediction_evolution.png")


def comprehensive_model_evaluation(checkpoint_path: str, data_path: str):
    """ç»¼åˆæ¨¡å‹è¯„ä¼°"""
    print("ğŸ” ç»¼åˆæ¨¡å‹è¯„ä¼°")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model, checkpoint = load_trained_model(checkpoint_path, device)
    
    # åŠ è½½æ•°æ®
    print("\\nåŠ è½½æµ‹è¯•æ•°æ®...")
    data_loader = TrajectoryDataLoader(
        data_path=data_path,
        batch_size=8,
        train_split=0.7,
        val_split=0.15,
        test_split=0.15,
        shuffle=False,
        num_workers=0,
        random_state=42
    )
    
    _, _, test_loader = data_loader.get_dataloaders()
    
    # è¿›è¡Œç»¼åˆè¯„ä¼°
    evaluation_results = evaluate_streaming_vs_batch(model, test_loader, device, detailed_analysis=True)
    
    # åˆ†æé¢„æµ‹æ¼”åŒ–
    analyze_timestep_evolution(evaluation_results['streaming_results'])
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    generate_evaluation_report(evaluation_results, checkpoint)
    
    return evaluation_results


def generate_evaluation_report(evaluation_results, checkpoint):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    print("\\nğŸ“„ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    
    batch_results = evaluation_results['batch_results']
    streaming_results = evaluation_results['streaming_results']
    
    report = f"""
# æµå¼HydraRocketæ¨¡å‹è¯„ä¼°æŠ¥å‘Š

## æ¨¡å‹ä¿¡æ¯
- è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'N/A')}
- éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('val_accuracy', 'N/A'):.4f}
- æ”¯æŒåºåˆ—é•¿åº¦: {checkpoint.get('supported_lengths', 'N/A')}

## æ€§èƒ½å¯¹æ¯”

### å‡†ç¡®ç‡
- æ‰¹é‡æ¨ç†å‡†ç¡®ç‡: {accuracy_score(batch_results['targets'], batch_results['predictions']):.4f}
- æµå¼æ¨ç†å‡†ç¡®ç‡: {accuracy_score(streaming_results['targets'], streaming_results['predictions']):.4f}

### æ•ˆç‡æŒ‡æ ‡
- æ—©æœŸåœæ­¢ç‡: {np.mean(streaming_results['early_stops']):.2%}
- å¹³å‡åœæ­¢æ—¶é—´æ­¥: {np.mean(streaming_results['stop_timesteps']):.1f}
- å¹³å‡ç½®ä¿¡åº¦: {np.mean(streaming_results['confidences']):.4f}

### æ—¶é—´æ€§èƒ½
- å¹³å‡æ‰¹é‡æ¨ç†æ—¶é—´: {np.mean(batch_results['inference_times'])*1000:.2f} ms/æ ·æœ¬
- å¹³å‡æµå¼æ¨ç†æ—¶é—´: {np.mean(streaming_results['inference_times'])*1000:.2f} ms/æ ·æœ¬

## è¯¦ç»†åˆ†ç±»æŠ¥å‘Š

### æ‰¹é‡æ¨ç†
{classification_report(batch_results['targets'], batch_results['predictions'])}

### æµå¼æ¨ç†  
{classification_report(streaming_results['targets'], streaming_results['predictions'])}

## ç»“è®º
æµå¼æ¨ç†ç³»ç»ŸæˆåŠŸå®ç°äº†é€æ­¥è¾“å…¥çš„å®æ—¶é¢„æµ‹åŠŸèƒ½ï¼Œåœ¨ä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶èƒ½å¤Ÿé€šè¿‡æ—©æœŸåœæ­¢æœºåˆ¶æé«˜æ¨ç†æ•ˆç‡ã€‚
"""
    
    with open('evaluation_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ° evaluation_report.md")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æµå¼HydraRocketæ¨¡å‹è¯„ä¼°ç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥æ–‡ä»¶
    checkpoint_path = "checkpoints/best_streaming_model.pth"
    data_path = "processed_data.npz"
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹")
        return
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬")
        return
    
    try:
        # è¿›è¡Œç»¼åˆè¯„ä¼°
        results = comprehensive_model_evaluation(checkpoint_path, data_path)
        
        print("\\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
        print("=" * 60)
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   - streaming_analysis.png (å¯¹æ¯”åˆ†æå›¾è¡¨)")
        print("   - prediction_evolution.png (é¢„æµ‹æ¼”åŒ–å›¾è¡¨)")
        print("   - evaluation_report.md (è¯¦ç»†è¯„ä¼°æŠ¥å‘Š)")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()