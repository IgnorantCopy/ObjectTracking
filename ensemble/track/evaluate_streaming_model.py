"""
ä¸“é—¨çš„æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”è„šæœ¬
"""
import time
import sys
import os
import re
import glob
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
sys.path.append(str(project_root))

import torch
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm

from ensemble.track.models.streaming_multi_rocket import StreamingMultiRocketClassifier, StreamingInferenceEngine
from ensemble.track.data_loader import TrajectoryDataLoader
from ensemble.track.configs.config import TRACK_COLUMNS, SEQ_LEN, DATA_ROOT
from ensemble.track.configs.streaming_config import StreamingConfig


def load_trained_model(checkpoint_path: str, config: StreamingConfig, data_info: Dict[str, Any]):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    device = config.device

    if device == 'auto':
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    elif device == 'cpu':
        device = torch.device("cpu")
    elif device == 'cuda':
        device = torch.device("cuda")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è®¾å¤‡: {device}")
    
    print(f"ä» {checkpoint_path} åŠ è½½æ¨¡å‹...")
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

    model = StreamingMultiRocketClassifier(
        c_in=data_info['num_features'],
        c_out=data_info['num_classes'],
        max_seq_len=data_info['seq_len'],
        num_features=config.num_features,
        dropout=config.dropout,
        confidence_threshold=config.confidence_threshold
    )
    
    # åŠ è½½æƒé‡
    model.load_state_dict(torch.load(checkpoint_path))
    model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"   æ”¯æŒçš„åºåˆ—é•¿åº¦: {model.supported_lengths}")
    print(f"   æ•°æ®ä¿¡æ¯: {data_info}")
    
    return model, device


def evaluate_streaming(model, data_loader, device, detailed_analysis=True):
    """å¯¹æ¯”æµå¼æ¨ç†å’Œæ‰¹é‡æ¨ç†çš„æ€§èƒ½"""
    print("=" * 50)
    
    model.eval()
    
    # å­˜å‚¨ç»“æœ
    streaming_results = {
        'predictions': [],
        'labels': [],
        'batch_ids': [],
        'begin_time': [],
        'rates': [],
        'stop_timesteps': [],
        'inference_times': [],
        'timestep_predictions': []  # æ¯ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹å†å²
    }
    engine = StreamingInferenceEngine(model)

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(data_loader, desc="è¯„ä¼°ä¸­")):
            sequences = batch['sequences'].to(device)  # (batch, seq_len, features)
            sequences = sequences.transpose(1, 2)
            labels = batch['labels'].to(device)
            batch_ids = batch['batch_ids']
            num_points = batch['num_points']
            streaming_results['batch_ids'].extend(batch_ids)

            # æµå¼æ¨ç†
            for i in range(sequences.shape[0]):
                engine.reset()
                seq = sequences[i]  # (features, seq_len)
                true_label = labels[i].item()

                start_time = time.time()
                timestep_preds = []
                # é€æ­¥æ·»åŠ æ—¶é—´æ­¥
                is_begin = False
                for t in range(1, seq.shape[1] + 1):
                    features = seq[:, :t]
                    result = engine.add_timestep(features, num_points[i])

                    prediction = result['prediction']
                    seq[-1, t - 1] = prediction
                    timestep_preds.append(prediction)

                    if true_label == prediction and not is_begin:
                        is_begin = True
                        streaming_results['begin_time'].append(t + 1)

                if not is_begin:
                    streaming_results['begin_time'].append(len(seq))
                streaming_time = time.time() - start_time

                result = engine.get_final_prediction()
                final_pred = result['prediction']
                stop_step = result['stop_timestep']
                rate = result['rate']

                streaming_results['predictions'].append(final_pred)
                streaming_results['labels'].append(true_label)
                streaming_results['rates'].append(rate)
                streaming_results['stop_timesteps'].append(stop_step)
                streaming_results['inference_times'].append(streaming_time)
                streaming_results['timestep_predictions'].append(timestep_preds)

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    streaming_accuracies = []
    predictions = np.array(streaming_results['predictions'])
    labels = np.array(streaming_results['labels'])
    rates = np.array(streaming_results['rates'])
    timestep_predictions = np.array(streaming_results['timestep_predictions'])
    for t in range(sequences.shape[2]):
        pred_t = timestep_predictions[:, t]
        streaming_accuracies.append(accuracy_score(labels, pred_t))
    avg_acc = 0
    for i in range(len(labels)):
        if predictions[i] == labels[i] and rates[i] >= 0.9:
            avg_acc += 1
    avg_acc /= len(labels)

    streaming_prec, streaming_rec, streaming_f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted'
    )
    
    # æ—¶é—´ç»Ÿè®¡
    avg_streaming_time = np.mean(streaming_results['inference_times']) * 1000  # ms
    
    # æ—©æœŸåœæ­¢ç»Ÿè®¡
    avg_stop_timestep = np.mean(streaming_results['stop_timesteps'])
    avg_begin_timestep = np.mean(streaming_results['begin_time'])
    max_timesteps = max(streaming_results['stop_timesteps'])
    
    print(f"{'æŒ‡æ ‡':<20} {'æµå¼æ¨ç†':<15}")
    print("-" * 65)
    print(f"{'å‡†ç¡®ç‡':<20} {avg_acc:<15.4f}")
    print(f"{'ç²¾ç¡®ç‡':<20} {streaming_prec:<15.4f}")
    print(f"{'å¬å›ç‡':<20} {streaming_rec:<15.4f}")
    print(f"{'F1åˆ†æ•°':<20} {streaming_f1:<15.4f}")
    print(f"{'æ¨ç†æ—¶é—´(ms)':<20} {avg_streaming_time:<15.2f}")
    print(f"{'å¹³å‡å¼€å§‹æ—¶é—´æ­¥':<20} {avg_begin_timestep:.1f}")
    print(f"{'å¹³å‡åœæ­¢æ—¶é—´æ­¥':<20} {avg_stop_timestep:.1f} / {max_timesteps}")

    if detailed_analysis:
        # è¯¦ç»†åˆ†æ
        fig = go.Figure(data=[
            go.Scatter(x=np.arange(1, len(streaming_accuracies)+1), y=streaming_accuracies, mode='lines')
        ])
        fig.update_layout(
            title="å‡†ç¡®ç‡éšæ—¶é—´æ­¥å˜åŒ–",
            xaxis_title="æ—¶é—´æ­¥",
            yaxis_title="å‡†ç¡®ç‡",
            legend_title="å‡†ç¡®ç‡"
        )
        fig.show()

    return {
        'streaming_results': streaming_results,
        'metrics': {
            'streaming_accuracy': streaming_prec,
            'avg_stop_timestep': avg_stop_timestep,
        }
    }


def export_results(results: Dict[str, Any], output_path: str, data_root: str):
    """å¯¼å‡ºç»“æœ"""
    os.makedirs(output_path, exist_ok=True)
    timestep_predictions = results['streaming_results']['timestep_predictions']
    batch_ids = results['streaming_results']['batch_ids']
    track_files = glob.glob(os.path.join(data_root, "èˆªè¿¹/Tracks_*.txt"))
    for track_file in track_files:
        match_result = re.match(r"Tracks_(\d+)_(\d+)_(\d+)\.txt", os.path.basename(track_file))
        batch_id = match_result.group(1)
        num_points = int(match_result.group(3))
        if batch_id in batch_ids:
            df = pd.read_csv(track_file, encoding='gbk', header=0, names=TRACK_COLUMNS)
            timestep_prediction = timestep_predictions[batch_ids.index(batch_id)]
            if num_points <= SEQ_LEN:
                timestep_prediction = timestep_prediction[:num_points]
            else:
                timestep_prediction.extend([timestep_prediction[-1]] * (num_points - SEQ_LEN))
            df['è¯†åˆ«ç»“æœ'] = np.array(timestep_prediction) + 1
            df.to_csv(os.path.join(output_path, os.path.basename(track_file)), index=False, encoding='gbk')
    print(f"âœ… ç»“æœå¯¼å‡ºæˆåŠŸ: {output_path}")


def comprehensive_model_evaluation(checkpoint_path: str):
    """ç»¼åˆæ¨¡å‹è¯„ä¼°"""
    print("ğŸ” ç»¼åˆæ¨¡å‹è¯„ä¼°")
    print("=" * 60)

    config = StreamingConfig()

    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    data_loader = TrajectoryDataLoader(
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        train_split=config.train_split,
        val_split=config.val_split,
        test_split=config.test_split,
        test_only=False,
        random_state=42,
    )

    _, _, test_loader = data_loader.get_dataloaders()
    data_info = data_loader.data_info

    model, device = load_trained_model(checkpoint_path, config, data_info)

    # è¿›è¡Œç»¼åˆè¯„ä¼°
    evaluation_results = evaluate_streaming(model, test_loader, device, detailed_analysis=False)
    
    return evaluation_results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æµå¼MultiRocketæ¨¡å‹è¯„ä¼°ç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥æ–‡ä»¶
    checkpoint_path = "./checkpoints/model_state_dict.pth"

    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹")
        return
    
    result = comprehensive_model_evaluation(checkpoint_path)
    export_results(result, os.path.join(DATA_ROOT, "test_results"), DATA_ROOT)


if __name__ == "__main__":
    main()