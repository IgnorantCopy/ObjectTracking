{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. 背景与问题\n",
    "\n",
    "数据加载器 `FusedDataset` 会加载一个样本的全部可用数据（所有帧和轨迹点），然后通过填充或截断的方式将序列统一到固定长度，再送入模型进行训练。\n",
    "然而，在实际的应用中，数据是逐点、依次到达的。我们的目标是让模型在每个时间点，仅利用当前和过去的信息，尽可能早地识别出目标类别。这两种模式之间存在偏差：\n",
    "**信息泄露**：离线训练模式让模型在训练时看到了“未来”的数据，这可能导致它学习到依赖序列后期信息的“捷径”，从而在面对只有早期数据的真实推理场景时表现不佳。\n",
    "**目标不一致**：训练目标（分类完整片段）与应用目标（尽早识别）不完全一致。\n",
    "\n",
    "## 2. 解决方案：基于前缀的流式训练策略\n",
    "为提升模型在流式推理任务中的泛化能力，引入了一种基于前缀采样的训练策略。具体做法是：在训练过程中，模型仅接收输入序列的随机长度前缀，以此模拟实际部署环境中“未来信息不可用”的约束条件。\n",
    "该方法能够有效缓解训练推理不一致（train-test mismatch）的问题，并显著提升模型在早期阶段的预测质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 代码修改建议\n",
    "\n",
    "创建一个新的数据集类 `FusedDatasetCausal`。核心改动在 `__getitem__` 方法中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 从 fusion/utils/dataset.py 中的基础类定义\n",
    "@dataclass\n",
    "class BatchFile:\n",
    "    \"\"\"批次文件信息\"\"\"\n",
    "    batch_num: int           # 航迹批号\n",
    "    label: int              # 目标类型标签\n",
    "    raw_file: str          # 原始回波文件路径\n",
    "    point_file: str        # 点迹文件路径\n",
    "    track_file: str        # 航迹文件路径\n",
    "\n",
    "class OriginalFusedDataset(Dataset):\n",
    "    \"\"\"这是原始的 FusedDataset 实现，作为对比。\"\"\"\n",
    "    def __init__(self, batch_files: list[BatchFile], image_seq_len=64, track_seq_len=20):\n",
    "        super().__init__()\n",
    "        self.batch_files = batch_files\n",
    "        self.image_seq_len = image_seq_len\n",
    "        self.track_seq_len = track_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_files)\n",
    "\n",
    "    def _load_data(self, batch_file):\n",
    "        # 这是一个模拟函数，实际应调用原有的数据加载逻辑\n",
    "        # 假设它返回了变长的图像和轨迹序列\n",
    "        # 随机生成一个长度，模拟真实数据的可变长度\n",
    "        seq_len = random.randint(10, 100) \n",
    "        print(f\"(模拟) 加载了批号 {batch_file.batch_num}，原始序列长度为: {seq_len}\")\n",
    "        images = torch.randn(seq_len, 1, 32, 544) # (T, C, H, W)\n",
    "        tracks = torch.randn(seq_len, 15) # (T, F)\n",
    "        return images, tracks\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        batch_file = self.batch_files[item]\n",
    "        cls = batch_file.label - 1\n",
    "        \n",
    "        # 加载完整的序列\n",
    "        images, tracks = self._load_data(batch_file)\n",
    "        # 演示，直接返回完整序列\n",
    "        return images, tracks, cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 修改后的 `FusedDatasetCausal` 实现\n",
    "\n",
    "以下是修改后的版本。关键改动在 `__getitem__` 方法中：它在加载了完整数据后，会随机选择一个切片点 `t`，并只返回序列的前 `t` 个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedDatasetCausal(Dataset):\n",
    "    def __init__(self, batch_files: list[BatchFile], image_seq_len=64, track_seq_len=20):\n",
    "        super().__init__()\n",
    "        self.batch_files = batch_files\n",
    "        self.image_seq_len = image_seq_len\n",
    "        self.track_seq_len = track_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_files)\n",
    "\n",
    "    def _load_data(self, batch_file):\n",
    "        # 这是一个模拟函数，与上面相同\n",
    "        seq_len = random.randint(10, 100)\n",
    "        print(f\"(模拟) 加载了批号 {batch_file.batch_num}，原始序列长度为: {seq_len}\")\n",
    "        images = torch.randn(seq_len, 1, 32, 544)\n",
    "        tracks = torch.randn(seq_len, 15)\n",
    "        return images, tracks\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        batch_file = self.batch_files[item]\n",
    "        cls = batch_file.label - 1\n",
    "\n",
    "        # 1. 加载完整的序列\n",
    "        images, tracks = self._load_data(batch_file)\n",
    "        original_len = images.shape[0]\n",
    "\n",
    "        # 2. 随机选择一个切片点 t (至少为1)\n",
    "        slice_point_t = random.randint(1, original_len)\n",
    "        print(f\"  -> 应用因果训练: 随机截取前 {slice_point_t} 帧数据进行训练。\")\n",
    "\n",
    "        # 3. 截取增量序列\n",
    "        images_sliced = images[:slice_point_t]\n",
    "        tracks_sliced = tracks[:slice_point_t]\n",
    "\n",
    "        # 4. 返回截取后的序列和最终的标签\n",
    "        # collate_fn 之后会负责将这个可变长度的短序列填充到固定长度\n",
    "        return images_sliced, tracks_sliced, cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 用法演示\n",
    "实际使用时，只需在 `train.py` 中将 `dataset.FusedDataset` 替换为 `dataset.FusedDatasetCausal` 即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一些虚拟的批次文件用于演示\n",
    "dummy_batch_files = [\n",
    "    BatchFile(batch_num=101, label=1, raw_file=\"\", point_file=\"\", track_file=\"\"),\n",
    "    BatchFile(batch_num=102, label=2, raw_file=\"\", point_file=\"\", track_file=\"\"),\n",
    "]\n",
    "\n",
    "# 实例化新的数据集\n",
    "causal_dataset = FusedDatasetCausal(dummy_batch_files)\n",
    "\n",
    "# 获取一个样本来观察效果\n",
    "print(\"--- 获取第一个样本 ---\")\n",
    "images, tracks, label = causal_dataset[0]\n",
    "print(f\"返回的图像序列形状: {images.shape}\")\n",
    "print(f\"返回的轨迹序列形状: {tracks.shape}\")\n",
    "print(f\"返回的标签: {label}\")\n",
    "\n",
    "print(\"\\n--- 获取第二个样本 ---\")\n",
    "images, tracks, label = causal_dataset[1]\n",
    "print(f\"返回的图像序列形状: {images.shape}\")\n",
    "print(f\"返回的轨迹序列形状: {tracks.shape}\")\n",
    "print(f\"返回的标签: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
